{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7242e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Load Libraries\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623f1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional but useful\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c1f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2. Load Dataset\n",
    "# ============================================\n",
    "df = pd.read_csv(\"heart_disease.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bdde3a",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2f9f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  Diabetes  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  \\\n",
      "0                   0.0     1.0       1.0        1.0  40.0     1.0     0.0       0.0           0.0     0.0      1.0                0.0            1.0          0.0      5.0      18.0      15.0   \n",
      "1                   0.0     0.0       0.0        0.0  25.0     1.0     0.0       0.0           1.0     0.0      0.0                0.0            0.0          1.0      3.0       0.0       0.0   \n",
      "2                   0.0     1.0       1.0        1.0  28.0     0.0     0.0       0.0           0.0     1.0      0.0                0.0            1.0          1.0      5.0      30.0      30.0   \n",
      "3                   0.0     1.0       0.0        1.0  27.0     0.0     0.0       0.0           1.0     1.0      1.0                0.0            1.0          0.0      2.0       0.0       0.0   \n",
      "4                   0.0     1.0       1.0        1.0  24.0     0.0     0.0       0.0           1.0     1.0      1.0                0.0            1.0          0.0      2.0       3.0       0.0   \n",
      "\n",
      "   DiffWalk  Sex   Age  Education  Income  \n",
      "0       1.0  0.0   9.0        4.0     3.0  \n",
      "1       0.0  0.0   7.0        6.0     1.0  \n",
      "2       1.0  0.0   9.0        4.0     8.0  \n",
      "3       0.0  0.0  11.0        3.0     6.0  \n",
      "4       0.0  0.0  11.0        5.0     4.0  \n",
      "Number of rows: 253680\n",
      "Number of columns: 22\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Basic Dataset Information\n",
    "# ============================================\n",
    "\n",
    "# First rows\n",
    "print(df.head())\n",
    "\n",
    "# Dimensions\n",
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f4417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HeartDiseaseorAttack', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 253680 entries, 0 to 253679\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   HeartDiseaseorAttack  253476 non-null  float64\n",
      " 1   HighBP                253452 non-null  float64\n",
      " 2   HighChol              253467 non-null  float64\n",
      " 3   CholCheck             253467 non-null  float64\n",
      " 4   BMI                   253470 non-null  float64\n",
      " 5   Smoker                253445 non-null  float64\n",
      " 6   Stroke                253482 non-null  float64\n",
      " 7   Diabetes              253457 non-null  float64\n",
      " 8   PhysActivity          253464 non-null  float64\n",
      " 9   Fruits                253466 non-null  float64\n",
      " 10  Veggies               253460 non-null  float64\n",
      " 11  HvyAlcoholConsump     253460 non-null  float64\n",
      " 12  AnyHealthcare         253455 non-null  float64\n",
      " 13  NoDocbcCost           253451 non-null  float64\n",
      " 14  GenHlth               253467 non-null  float64\n",
      " 15  MentHlth              253464 non-null  float64\n",
      " 16  PhysHlth              253477 non-null  float64\n",
      " 17  DiffWalk              253449 non-null  float64\n",
      " 18  Sex                   253450 non-null  float64\n",
      " 19  Age                   253468 non-null  float64\n",
      " 20  Education             253469 non-null  float64\n",
      " 21  Income                253452 non-null  float64\n",
      "dtypes: float64(22)\n",
      "memory usage: 42.6 MB\n",
      "None\n",
      "                         count       mean       std   min   25%   50%   75%   max\n",
      "HeartDiseaseorAttack  253476.0   0.094186  0.292088   0.0   0.0   0.0   0.0   1.0\n",
      "HighBP                253452.0   0.429008  0.494935   0.0   0.0   0.0   1.0   1.0\n",
      "HighChol              253467.0   0.424114  0.494209   0.0   0.0   0.0   1.0   1.0\n",
      "CholCheck             253467.0   0.962666  0.189580   0.0   1.0   1.0   1.0   1.0\n",
      "BMI                   253470.0  28.382467  6.609107  12.0  24.0  27.0  31.0  98.0\n",
      "Smoker                253445.0   0.443134  0.496757   0.0   0.0   0.0   1.0   1.0\n",
      "Stroke                253482.0   0.040571  0.197294   0.0   0.0   0.0   0.0   1.0\n",
      "Diabetes              253457.0   0.296914  0.698147   0.0   0.0   0.0   0.0   2.0\n",
      "PhysActivity          253464.0   0.756518  0.429185   0.0   1.0   1.0   1.0   1.0\n",
      "Fruits                253466.0   0.634255  0.481639   0.0   0.0   1.0   1.0   1.0\n",
      "Veggies               253460.0   0.811406  0.391186   0.0   1.0   1.0   1.0   1.0\n",
      "HvyAlcoholConsump     253460.0   0.056198  0.230305   0.0   0.0   0.0   0.0   1.0\n",
      "AnyHealthcare         253455.0   0.951072  0.215718   0.0   1.0   1.0   1.0   1.0\n",
      "NoDocbcCost           253451.0   0.084190  0.277673   0.0   0.0   0.0   0.0   1.0\n",
      "GenHlth               253467.0   2.511376  1.068481   1.0   2.0   2.0   3.0   5.0\n",
      "MentHlth              253464.0   3.185506  7.413680   0.0   0.0   0.0   2.0  30.0\n",
      "PhysHlth              253477.0   4.242156  8.718150   0.0   0.0   0.0   3.0  30.0\n",
      "DiffWalk              253449.0   0.168227  0.374069   0.0   0.0   0.0   0.0   1.0\n",
      "Sex                   253450.0   0.440383  0.496434   0.0   0.0   0.0   1.0   1.0\n",
      "Age                   253468.0   8.032126  3.054265   1.0   6.0   8.0  10.0  13.0\n",
      "Education             253469.0   5.050515  0.985725   1.0   4.0   5.0   6.0   6.0\n",
      "Income                253452.0   6.053852  2.071116   1.0   5.0   7.0   8.0   8.0\n"
     ]
    }
   ],
   "source": [
    "# Column names\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Data types & structure\n",
    "print(df.info())\n",
    "\n",
    "# Summary statistics\n",
    "print(df.describe(include=\"all\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abc1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "HeartDiseaseorAttack    204\n",
      "HighBP                  228\n",
      "HighChol                213\n",
      "CholCheck               213\n",
      "BMI                     210\n",
      "Smoker                  235\n",
      "Stroke                  198\n",
      "Diabetes                223\n",
      "PhysActivity            216\n",
      "Fruits                  214\n",
      "Veggies                 220\n",
      "HvyAlcoholConsump       220\n",
      "AnyHealthcare           225\n",
      "NoDocbcCost             229\n",
      "GenHlth                 213\n",
      "MentHlth                216\n",
      "PhysHlth                203\n",
      "DiffWalk                231\n",
      "Sex                     230\n",
      "Age                     212\n",
      "Education               211\n",
      "Income                  228\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 4792\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. Missing Value Check\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nTotal missing values:\", df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391566e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 23697\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. Duplicate Row Check\n",
    "# ============================================\n",
    "\n",
    "num_dups = df.duplicated().sum()\n",
    "print(\"\\nNumber of duplicate rows:\", num_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a883acfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric Columns:\n",
      " ['HeartDiseaseorAttack', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n",
      "\n",
      "Categorical Columns:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. Identify Numeric & Categorical Variables\n",
    "# ============================================\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric Columns:\\n\", numeric_cols)\n",
    "print(\"\\nCategorical Columns:\\n\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d83906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HeartDiseaseorAttack counts:\n",
      "HeartDiseaseorAttack\n",
      "0.0    229602\n",
      "1.0     23874\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Proportion:\n",
      "HeartDiseaseorAttack\n",
      "0.0    0.905814\n",
      "1.0    0.094186\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. Distribution of Key Variables\n",
    "# ============================================\n",
    "\n",
    "# Heart disease count\n",
    "print(\"\\nHeartDiseaseorAttack counts:\")\n",
    "print(df[\"HeartDiseaseorAttack\"].value_counts())\n",
    "\n",
    "# Proportion\n",
    "print(\"\\nProportion:\")\n",
    "print(df[\"HeartDiseaseorAttack\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28e44b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BMI Summary:\n",
      "count    253470.000000\n",
      "mean         28.382467\n",
      "std           6.609107\n",
      "min          12.000000\n",
      "25%          24.000000\n",
      "50%          27.000000\n",
      "75%          31.000000\n",
      "max          98.000000\n",
      "Name: BMI, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BMI statistics\n",
    "print(\"\\nBMI Summary:\")\n",
    "print(df[\"BMI\"].describe())\n",
    "\n",
    "# BMI histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df[\"BMI\"], bins=30, color=\"skyblue\", edgecolor=\"white\")\n",
    "plt.title(\"BMI Distribution\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e57f9",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd3ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows before dropping missing values: 253680\n",
      "HeartDiseaseorAttack    204\n",
      "HighBP                  228\n",
      "HighChol                213\n",
      "CholCheck               213\n",
      "BMI                     210\n",
      "Smoker                  235\n",
      "Stroke                  198\n",
      "Diabetes                223\n",
      "PhysActivity            216\n",
      "Fruits                  214\n",
      "Veggies                 220\n",
      "HvyAlcoholConsump       220\n",
      "AnyHealthcare           225\n",
      "NoDocbcCost             229\n",
      "GenHlth                 213\n",
      "MentHlth                216\n",
      "PhysHlth                203\n",
      "DiffWalk                231\n",
      "Sex                     230\n",
      "Age                     212\n",
      "Education               211\n",
      "Income                  228\n",
      "dtype: int64\n",
      "\n",
      "Rows containing at least one missing value: 1358\n",
      "Rows after dropping missing values: 252322\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# Missing Value Handling\n",
    "print(\"\\nRows before dropping missing values:\", df_clean.shape[0])\n",
    "print(df_clean.isna().sum())\n",
    "\n",
    "rows_with_na = df_clean.isna().any(axis=1).sum()\n",
    "print(\"\\nRows containing at least one missing value:\", rows_with_na)\n",
    "\n",
    "df_clean = df_clean.dropna()\n",
    "print(\"Rows after dropping missing values:\", df_clean.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d5ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows: 23697\n",
      "Rows after removing duplicates: 228625\n"
     ]
    }
   ],
   "source": [
    "# Remove Duplicate Rows\n",
    "\n",
    "dup_n = df_clean.duplicated().sum()\n",
    "print(\"\\nDuplicate rows:\", dup_n)\n",
    "\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "print(\"Rows after removing duplicates:\", df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "975e1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BMI_capped summary:\n",
      "count    228625.000000\n",
      "mean         28.578515\n",
      "std           6.170311\n",
      "min          18.000000\n",
      "25%          24.000000\n",
      "50%          27.000000\n",
      "75%          32.000000\n",
      "max          50.000000\n",
      "Name: BMI_capped, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Handle Outliers for BMI (Winsorize 1–99 percentile)\n",
    "\n",
    "bmi_q_low = df_clean[\"BMI\"].quantile(0.01)\n",
    "bmi_q_high = df_clean[\"BMI\"].quantile(0.99)\n",
    "\n",
    "df_clean[\"BMI_capped\"] = df_clean[\"BMI\"].clip(lower=bmi_q_low, upper=bmi_q_high)\n",
    "\n",
    "print(\"\\nBMI_capped summary:\")\n",
    "print(df_clean[\"BMI_capped\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f762f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode GenHlth → Ordered Factor\n",
    "\n",
    "genhlth_map = {\n",
    "    1: \"Excellent\",\n",
    "    2: \"Very Good\",\n",
    "    3: \"Good\",\n",
    "    4: \"Fair\",\n",
    "    5: \"Poor\"\n",
    "}\n",
    "\n",
    "df_clean[\"GenHlth_Factor\"] = df_clean[\"GenHlth\"].map(genhlth_map)\n",
    "df_clean[\"GenHlth_Factor\"] = pd.Categorical(\n",
    "    df_clean[\"GenHlth_Factor\"],\n",
    "    categories=[\"Excellent\", \"Very Good\", \"Good\", \"Fair\", \"Poor\"],\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e175b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with MentHlth > 30: 0\n",
      "Rows with PhysHlth > 30: 0\n",
      "Total invalid rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Validate MentHlth & PhysHlth ranges (0–30)\n",
    "\n",
    "invalid_ment = (df_clean[\"MentHlth\"] > 30).sum()\n",
    "invalid_phys = (df_clean[\"PhysHlth\"] > 30).sum()\n",
    "\n",
    "print(\"Rows with MentHlth > 30:\", invalid_ment)\n",
    "print(\"Rows with PhysHlth > 30:\", invalid_phys)\n",
    "\n",
    "invalid_total = ((df_clean[\"MentHlth\"] > 30) | \n",
    "                 (df_clean[\"PhysHlth\"] > 30)).sum()\n",
    "\n",
    "print(\"Total invalid rows:\", invalid_total)\n",
    "\n",
    "df_clean = df_clean[(df_clean[\"MentHlth\"] <= 30) &\n",
    "                    (df_clean[\"PhysHlth\"] <= 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae8b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BMI BMI_Category\n",
      "0  40.0        Obese\n",
      "1  25.0       Normal\n",
      "2  28.0   Overweight\n",
      "3  27.0   Overweight\n",
      "4  24.0       Normal\n",
      "5  25.0       Normal\n",
      "6  30.0   Overweight\n",
      "7  25.0       Normal\n",
      "8  30.0   Overweight\n",
      "9  24.0       Normal\n"
     ]
    }
   ],
   "source": [
    "# BMI Category (cut)\n",
    "\n",
    "df_clean[\"BMI_Category\"] = pd.cut(\n",
    "    df_clean[\"BMI\"],\n",
    "    bins=[-np.inf, 18.5, 25, 30, np.inf],\n",
    "    labels=[\"Underweight\", \"Normal\", \"Overweight\", \"Obese\"]\n",
    ")\n",
    "\n",
    "print(df_clean[[\"BMI\", \"BMI_Category\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69661b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age AgeGroup\n",
      "0    9    60-64\n",
      "1    7    50-54\n",
      "2    9    60-64\n",
      "3   11    70-74\n",
      "4   11    70-74\n",
      "5   10    65-69\n",
      "6    9    60-64\n",
      "7   11    70-74\n",
      "8    9    60-64\n",
      "9    8    55-59\n"
     ]
    }
   ],
   "source": [
    "# AgeGroup (mapping 1–13 → age ranges)\n",
    "\n",
    "df_clean[\"Age\"] = df_clean[\"Age\"].astype(int)\n",
    "\n",
    "age_labels = [\n",
    "    \"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\",\n",
    "    \"45-49\", \"50-54\", \"55-59\", \"60-64\",\n",
    "    \"65-69\", \"70-74\", \"75-79\", \"80+\"\n",
    "]\n",
    "\n",
    "df_clean[\"AgeGroup\"] = pd.Categorical(\n",
    "    df_clean[\"Age\"].apply(lambda x: age_labels[x - 1]),\n",
    "    categories=age_labels,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "print(df_clean[[\"Age\", \"AgeGroup\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b6f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age AgeBand\n",
      "0    9   50-64\n",
      "1    7   50-64\n",
      "2    9   50-64\n",
      "3   11   65-74\n",
      "4   11   65-74\n",
      "5   10   65-74\n",
      "6    9   50-64\n",
      "7   11   65-74\n",
      "8    9   50-64\n",
      "9    8   50-64\n"
     ]
    }
   ],
   "source": [
    "# AgeBand (broader groups)\n",
    "\n",
    "def map_ageband(code):\n",
    "    if code <= 3:\n",
    "        return \"18-34\"\n",
    "    elif code <= 6:\n",
    "        return \"35-49\"\n",
    "    elif code <= 9:\n",
    "        return \"50-64\"\n",
    "    elif code <= 11:\n",
    "        return \"65-74\"\n",
    "    else:\n",
    "        return \"75+\"\n",
    "\n",
    "df_clean[\"AgeBand\"] = df_clean[\"Age\"].apply(map_ageband)\n",
    "df_clean[\"AgeBand\"] = pd.Categorical(\n",
    "    df_clean[\"AgeBand\"],\n",
    "    categories=[\"18-34\", \"35-49\", \"50-64\", \"65-74\", \"75+\"],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "print(df_clean[[\"Age\", \"AgeBand\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5212d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Smoker  HvyAlcoholConsump  PhysActivity  Fruits  Veggies  RiskScore\n",
      "0     1.0                0.0           0.0     0.0      1.0          3\n",
      "1     1.0                0.0           1.0     0.0      0.0          3\n",
      "2     0.0                0.0           0.0     1.0      0.0          2\n",
      "3     0.0                0.0           1.0     1.0      1.0          0\n",
      "4     0.0                0.0           1.0     1.0      1.0          0\n"
     ]
    }
   ],
   "source": [
    "# Lifestyle Risk Score\n",
    "\n",
    "df_clean[\"RiskScore\"] = (\n",
    "      df_clean[\"Smoker\"].astype(int)\n",
    "    + df_clean[\"HvyAlcoholConsump\"].astype(int)\n",
    "    + (1 - df_clean[\"PhysActivity\"].astype(int))\n",
    "    + (1 - df_clean[\"Fruits\"].astype(int))\n",
    "    + (1 - df_clean[\"Veggies\"].astype(int))\n",
    ")\n",
    "\n",
    "print(df_clean[[\"Smoker\", \"HvyAlcoholConsump\", \"PhysActivity\", \n",
    "                \"Fruits\", \"Veggies\", \"RiskScore\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c02361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HighBP  HighChol  Diabetes  Stroke  DiseaseCount\n",
      "0     1.0       1.0       0.0     0.0             2\n",
      "1     0.0       0.0       0.0     0.0             0\n",
      "2     1.0       1.0       0.0     0.0             2\n",
      "3     1.0       0.0       0.0     0.0             1\n",
      "4     1.0       1.0       0.0     0.0             2\n"
     ]
    }
   ],
   "source": [
    "# Disease Burden Index\n",
    "\n",
    "df_clean[\"DiseaseCount\"] = (\n",
    "      df_clean[\"HighBP\"].astype(int)\n",
    "    + df_clean[\"HighChol\"].astype(int)\n",
    "    + df_clean[\"Diabetes\"].astype(int)\n",
    "    + df_clean[\"Stroke\"].astype(int)\n",
    ")\n",
    "\n",
    "print(df_clean[[\"HighBP\", \"HighChol\", \"Diabetes\", \"Stroke\", \n",
    "                \"DiseaseCount\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca481fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MentHlth  PhysHlth  HealthStressIndex\n",
      "0      18.0      15.0               33.0\n",
      "1       0.0       0.0                0.0\n",
      "2      30.0      30.0               60.0\n",
      "3       0.0       0.0                0.0\n",
      "4       3.0       0.0                3.0\n"
     ]
    }
   ],
   "source": [
    "# Health Stress Index\n",
    "\n",
    "df_clean[\"HealthStressIndex\"] = df_clean[\"MentHlth\"] + df_clean[\"PhysHlth\"]\n",
    "\n",
    "print(df_clean[[\"MentHlth\", \"PhysHlth\", \"HealthStressIndex\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a50b9a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AnyHealthcare  NoDocbcCost  HealthcareScore\n",
      "0            1.0          0.0                2\n",
      "1            0.0          1.0                0\n",
      "2            1.0          1.0                1\n",
      "3            1.0          0.0                2\n",
      "4            1.0          0.0                2\n"
     ]
    }
   ],
   "source": [
    "# Healthcare Access Score\n",
    "\n",
    "df_clean[\"HealthcareScore\"] = (\n",
    "    df_clean[\"AnyHealthcare\"].astype(int)\n",
    "    + (1 - df_clean[\"NoDocbcCost\"].astype(int))\n",
    ")\n",
    "\n",
    "print(df_clean[[\"AnyHealthcare\", \"NoDocbcCost\", \"HealthcareScore\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "845dd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BMI  ObeseFlag\n",
      "0  40.0          1\n",
      "1  25.0          0\n",
      "2  28.0          0\n",
      "3  27.0          0\n",
      "4  24.0          0\n"
     ]
    }
   ],
   "source": [
    "# Obesity Indicator\n",
    "\n",
    "df_clean[\"ObeseFlag\"] = (df_clean[\"BMI\"] >= 30).astype(int)\n",
    "\n",
    "print(df_clean[[\"BMI\", \"ObeseFlag\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8da523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RiskScore LifestyleProfile\n",
      "0          3     ModerateRisk\n",
      "1          3     ModerateRisk\n",
      "2          2     ModerateRisk\n",
      "3          0          Healthy\n",
      "4          0          Healthy\n"
     ]
    }
   ],
   "source": [
    "# 8.14 Lifestyle Profile Category\n",
    "\n",
    "def map_lifestyle(x):\n",
    "    if x <= 1:\n",
    "        return \"Healthy\"\n",
    "    elif x <= 3:\n",
    "        return \"ModerateRisk\"\n",
    "    else:\n",
    "        return \"HighRisk\"\n",
    "\n",
    "df_clean[\"LifestyleProfile\"] = df_clean[\"RiskScore\"].apply(map_lifestyle)\n",
    "\n",
    "print(df_clean[[\"RiskScore\", \"LifestyleProfile\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70c49094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"heart_disease_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488b677",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f3f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da048ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59361f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2572aa1",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d9a062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['AgeGroup', 'AgeBand', 'LifestyleProfile']\n",
      "Numeric columns: ['Age', 'Sex', 'HighBP', 'HighChol', 'Diabetes', 'Stroke', 'Smoker', 'PhysActivity', 'PhysHlth', 'MentHlth', 'HealthStressIndex', 'DiseaseCount', 'ObeseFlag', 'RiskScore', 'BMI']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ASSUMPTION:\n",
    "# df_clean already exists from your preprocessing (Part 1)\n",
    "# and contains all engineered features:\n",
    "#   AgeGroup, AgeBand, RiskScore, DiseaseCount, HealthStressIndex,\n",
    "#   HealthcareScore, ObeseFlag, LifestyleProfile, etc.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Just in case: ensure Age is integer (for AgeGroup/AgeBand logic)\n",
    "df_clean[\"Age\"] = df_clean[\"Age\"].astype(int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FEATURES & TARGET FROM PREPROCESSED df_clean\n",
    "# =============================================================================\n",
    "\n",
    "feature_cols = [\n",
    "    \"Age\", \"AgeGroup\", \"AgeBand\",\n",
    "    \"Sex\",\n",
    "    \"HighBP\", \"HighChol\", \"Diabetes\", \"Stroke\",\n",
    "    \"Smoker\", \"PhysActivity\",\n",
    "    \"PhysHlth\", \"MentHlth\", \"HealthStressIndex\",\n",
    "    \"DiseaseCount\", \"ObeseFlag\",\n",
    "    \"RiskScore\", \"LifestyleProfile\",\n",
    "    \"BMI\"\n",
    "]\n",
    "\n",
    "target_col = \"HeartDiseaseorAttack\"\n",
    "\n",
    "X = df_clean[feature_cols].copy()\n",
    "y = df_clean[target_col].copy()\n",
    "\n",
    "# Identify categorical columns for one-hot encoding\n",
    "categorical_cols = [\"AgeGroup\", \"AgeBand\", \"LifestyleProfile\"]\n",
    "numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48bb702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes:\n",
      "X_train: (182900, 18)\n",
      "X_val: (22862, 18)\n",
      "X_test: (22863, 18)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. Train / Validation / Test Split (80/20 then 50/50)\n",
    "# =============================================================================\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=123,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=123,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f44dd10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded train shape: (182900, 36)\n",
      "\n",
      "After SMOTE: [164027 164027]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. Preprocessing Pipeline (OneHot + Scaling)\n",
    "# =============================================================================\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit ONLY on training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_val_enc   = preprocessor.transform(X_val)\n",
    "X_test_enc  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nEncoded train shape:\", X_train_enc.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. APPLY SMOTE (ONLY on training)\n",
    "# =============================================================================\n",
    "\n",
    "sm = SMOTE(random_state=123)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_enc, y_train)\n",
    "\n",
    "print(\"\\nAfter SMOTE:\", np.bincount(y_train_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec432c4",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95ff43",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a11563e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Bayesian Optimization...\n",
      "\n",
      "|   iter    |  target   | learni... | num_le... | featur... | baggin... | min_ch... |\n",
      "-------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907405814256813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907405814256813\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8205259076331565, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8205259076331565\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907405814256813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907405814256813\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8205259076331565, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8205259076331565\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6907405814256813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6907405814256813\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8205259076331565, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8205259076331565\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.8068559\u001b[39m | \u001b[39m0.1423291\u001b[39m | \u001b[39m37.168360\u001b[39m | \u001b[39m0.6907405\u001b[39m | \u001b[39m0.8205259\u001b[39m | \u001b[39m45.973448\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8739318954339452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8739318954339452\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7923727605937444, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7923727605937444\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8739318954339452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8739318954339452\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7923727605937444, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7923727605937444\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8739318954339452, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8739318954339452\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7923727605937444, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7923727605937444\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.8049381\u001b[39m | \u001b[39m0.0903902\u001b[39m | \u001b[39m78.845851\u001b[39m | \u001b[39m0.8739318\u001b[39m | \u001b[39m0.7923727\u001b[39m | \u001b[39m29.605875\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7754288978718498, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7754288978718498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6238711586438274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6238711586438274\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7754288978718498, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7754288978718498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6238711586438274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6238711586438274\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7754288978718498, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7754288978718498\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6238711586438274, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6238711586438274\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.8060489\u001b[39m | \u001b[39m0.0752038\u001b[39m | \u001b[39m63.742982\u001b[39m | \u001b[39m0.7754288\u001b[39m | \u001b[39m0.6238711\u001b[39m | \u001b[39m29.902212\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.670180702458997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.670180702458997\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8126205495367353, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8126205495367353\n",
      "[LightGBM] [Warning] feature_fraction is set=0.670180702458997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.670180702458997\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8126205495367353, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8126205495367353\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.670180702458997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.670180702458997\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8126205495367353, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8126205495367353\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.8052316\u001b[39m | \u001b[39m0.1502191\u001b[39m | \u001b[39m30.949503\u001b[39m | \u001b[39m0.6701807\u001b[39m | \u001b[39m0.8126205\u001b[39m | \u001b[39m36.591379\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8897821299442541, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8897821299442541\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8444094042710332, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8444094042710332\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8897821299442541, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8897821299442541\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8444094042710332, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8444094042710332\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8897821299442541, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8897821299442541\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8444094042710332, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8444094042710332\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.8006759\u001b[39m | \u001b[39m0.1305361\u001b[39m | \u001b[39m70.965907\u001b[39m | \u001b[39m0.8897821\u001b[39m | \u001b[39m0.8444094\u001b[39m | \u001b[39m46.122169\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6913052923515822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6913052923515822\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7174856185555317, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7174856185555317\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6913052923515822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6913052923515822\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7174856185555317, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7174856185555317\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6913052923515822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6913052923515822\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7174856185555317, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7174856185555317\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.8078985\u001b[39m | \u001b[35m0.0713621\u001b[39m | \u001b[35m41.707319\u001b[39m | \u001b[35m0.6913052\u001b[39m | \u001b[35m0.7174856\u001b[39m | \u001b[35m41.548806\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7723451053318575, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7723451053318575\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974740390601225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974740390601225\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7723451053318575, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7723451053318575\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974740390601225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974740390601225\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7723451053318575, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7723451053318575\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974740390601225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974740390601225\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.8077571\u001b[39m | \u001b[39m0.0274999\u001b[39m | \u001b[39m46.022070\u001b[39m | \u001b[39m0.7723451\u001b[39m | \u001b[39m0.7974740\u001b[39m | \u001b[39m31.291514\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9573556652468539, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9573556652468539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9776640072815519, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9776640072815519\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9573556652468539, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9573556652468539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9776640072815519, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9776640072815519\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9573556652468539, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9573556652468539\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9776640072815519, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9776640072815519\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m0.8079527\u001b[39m | \u001b[35m0.0693296\u001b[39m | \u001b[35m45.581078\u001b[39m | \u001b[35m0.9573556\u001b[39m | \u001b[35m0.9776640\u001b[39m | \u001b[35m35.091833\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6094372820633912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6094372820633912\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7425861531831591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7425861531831591\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6094372820633912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6094372820633912\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7425861531831591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7425861531831591\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6094372820633912, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6094372820633912\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7425861531831591, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7425861531831591\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m0.8089713\u001b[39m | \u001b[35m0.0335407\u001b[39m | \u001b[35m58.733069\u001b[39m | \u001b[35m0.6094372\u001b[39m | \u001b[35m0.7425861\u001b[39m | \u001b[35m10.173513\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.8089604\u001b[39m | \u001b[39m0.0359033\u001b[39m | \u001b[39m75.772420\u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7781674489813843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7781674489813843\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9102765962462622, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9102765962462622\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7781674489813843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7781674489813843\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9102765962462622, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9102765962462622\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7781674489813843, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7781674489813843\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9102765962462622, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9102765962462622\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.8009878\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m37.590371\u001b[39m | \u001b[39m0.7781674\u001b[39m | \u001b[39m0.9102765\u001b[39m | \u001b[39m10.0     \u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9088583263438936, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088583263438936\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834365417826113, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834365417826113\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9088583263438936, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088583263438936\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834365417826113, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834365417826113\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9088583263438936, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9088583263438936\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8834365417826113, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8834365417826113\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8044043\u001b[39m | \u001b[39m0.0930795\u001b[39m | \u001b[39m66.850821\u001b[39m | \u001b[39m0.9088583\u001b[39m | \u001b[39m0.8834365\u001b[39m | \u001b[39m15.044185\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.800682442898596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.800682442898596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974064596363588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974064596363588\n",
      "[LightGBM] [Warning] feature_fraction is set=0.800682442898596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.800682442898596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974064596363588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974064596363588\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.800682442898596, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.800682442898596\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7974064596363588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7974064596363588\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.8013888\u001b[39m | \u001b[39m0.1616306\u001b[39m | \u001b[39m53.639974\u001b[39m | \u001b[39m0.8006824\u001b[39m | \u001b[39m0.7974064\u001b[39m | \u001b[39m16.125691\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8356987293360585, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8356987293360585\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7009396554914497, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7009396554914497\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8356987293360585, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8356987293360585\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7009396554914497, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7009396554914497\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8356987293360585, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8356987293360585\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7009396554914497, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7009396554914497\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.7922280\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m80.0     \u001b[39m | \u001b[39m0.8356987\u001b[39m | \u001b[39m0.7009396\u001b[39m | \u001b[39m10.792817\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6297773910928117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6297773910928117\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.953384458570571, subsample=1.0 will be ignored. Current value: bagging_fraction=0.953384458570571\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6297773910928117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6297773910928117\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.953384458570571, subsample=1.0 will be ignored. Current value: bagging_fraction=0.953384458570571\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6297773910928117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6297773910928117\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.953384458570571, subsample=1.0 will be ignored. Current value: bagging_fraction=0.953384458570571\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.8044911\u001b[39m | \u001b[39m0.1093691\u001b[39m | \u001b[39m59.074837\u001b[39m | \u001b[39m0.6297773\u001b[39m | \u001b[39m0.9533844\u001b[39m | \u001b[39m10.788734\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7613588423396208, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7613588423396208\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9785464581370213, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9785464581370213\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7613588423396208, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7613588423396208\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9785464581370213, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9785464581370213\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033408 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7613588423396208, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7613588423396208\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9785464581370213, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9785464581370213\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.8084417\u001b[39m | \u001b[39m0.0801328\u001b[39m | \u001b[39m37.719118\u001b[39m | \u001b[39m0.7613588\u001b[39m | \u001b[39m0.9785464\u001b[39m | \u001b[39m28.806251\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6570311201256346, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6570311201256346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7437442607059943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7437442607059943\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6570311201256346, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6570311201256346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7437442607059943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7437442607059943\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6570311201256346, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6570311201256346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7437442607059943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7437442607059943\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.8029010\u001b[39m | \u001b[39m0.1482281\u001b[39m | \u001b[39m64.463010\u001b[39m | \u001b[39m0.6570311\u001b[39m | \u001b[39m0.7437442\u001b[39m | \u001b[39m34.239943\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9994559470564415, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9994559470564415\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9760231888561499, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9760231888561499\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9994559470564415, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9994559470564415\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9760231888561499, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9760231888561499\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9994559470564415, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9994559470564415\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9760231888561499, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9760231888561499\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.8076056\u001b[39m | \u001b[39m0.0635195\u001b[39m | \u001b[39m23.386112\u001b[39m | \u001b[39m0.9994559\u001b[39m | \u001b[39m0.9760231\u001b[39m | \u001b[39m26.200038\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8088373963029151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8088373963029151\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9333180785194305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9333180785194305\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8088373963029151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8088373963029151\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9333180785194305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9333180785194305\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8088373963029151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8088373963029151\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9333180785194305, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9333180785194305\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.8072929\u001b[39m | \u001b[39m0.0763081\u001b[39m | \u001b[39m58.400346\u001b[39m | \u001b[39m0.8088373\u001b[39m | \u001b[39m0.9333180\u001b[39m | \u001b[39m41.941004\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9140863278469007, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9140863278469007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7556832810939343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7556832810939343\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9140863278469007, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9140863278469007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7556832810939343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7556832810939343\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9140863278469007, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9140863278469007\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7556832810939343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7556832810939343\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.8021151\u001b[39m | \u001b[39m0.1318918\u001b[39m | \u001b[39m56.429142\u001b[39m | \u001b[39m0.9140863\u001b[39m | \u001b[39m0.7556832\u001b[39m | \u001b[39m34.499577\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664968949427617, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664968949427617\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8390980324774369, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8390980324774369\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664968949427617, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664968949427617\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8390980324774369, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8390980324774369\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664968949427617, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664968949427617\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8390980324774369, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8390980324774369\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m0.8027752\u001b[39m | \u001b[39m0.1519421\u001b[39m | \u001b[39m57.723741\u001b[39m | \u001b[39m0.6649689\u001b[39m | \u001b[39m0.8390980\u001b[39m | \u001b[39m37.818649\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.905687298266942, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.905687298266942\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9073228773440023, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9073228773440023\n",
      "[LightGBM] [Warning] feature_fraction is set=0.905687298266942, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.905687298266942\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9073228773440023, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9073228773440023\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.905687298266942, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.905687298266942\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9073228773440023, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9073228773440023\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m0.7982752\u001b[39m | \u001b[39m0.1903274\u001b[39m | \u001b[39m60.465354\u001b[39m | \u001b[39m0.9056872\u001b[39m | \u001b[39m0.9073228\u001b[39m | \u001b[39m57.201534\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6145072891063371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145072891063371\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.982582766892725, subsample=1.0 will be ignored. Current value: bagging_fraction=0.982582766892725\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6145072891063371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145072891063371\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.982582766892725, subsample=1.0 will be ignored. Current value: bagging_fraction=0.982582766892725\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6145072891063371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145072891063371\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.982582766892725, subsample=1.0 will be ignored. Current value: bagging_fraction=0.982582766892725\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m0.8088318\u001b[39m | \u001b[39m0.0256099\u001b[39m | \u001b[39m75.684598\u001b[39m | \u001b[39m0.6145072\u001b[39m | \u001b[39m0.9825827\u001b[39m | \u001b[39m10.292373\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "| \u001b[35m24       \u001b[39m | \u001b[35m0.8095059\u001b[39m | \u001b[35m0.01     \u001b[39m | \u001b[35m57.551843\u001b[39m | \u001b[35m0.6      \u001b[39m | \u001b[35m0.6      \u001b[39m | \u001b[35m10.0     \u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7927061506907918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7927061506907918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8017207393616116, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8017207393616116\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7927061506907918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7927061506907918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8017207393616116, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8017207393616116\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7927061506907918, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7927061506907918\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8017207393616116, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8017207393616116\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m0.8076493\u001b[39m | \u001b[39m0.0832137\u001b[39m | \u001b[39m45.047759\u001b[39m | \u001b[39m0.7927061\u001b[39m | \u001b[39m0.8017207\u001b[39m | \u001b[39m33.147357\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m0.8080959\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m47.232616\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.412216\u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m0.7923825\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m73.735853\u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m10.0     \u001b[39m |\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m0.8082170\u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m76.740623\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m10.440515\u001b[39m |\n",
      "=====================================================================================\n",
      "\n",
      "Best params: {'learning_rate': 0.01, 'num_leaves': 57, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10}\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 164027, number of negative: 164027\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8528\n",
      "[LightGBM] [Info] Number of data points in the train set: 328054, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(bagging_fraction=0.6, feature_fraction=0.6, learning_rate=0.01,\n",
       "               min_child_samples=10, n_estimators=600, n_jobs=-1, num_leaves=57,\n",
       "               random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(bagging_fraction=0.6, feature_fraction=0.6, learning_rate=0.01,\n",
       "               min_child_samples=10, n_estimators=600, n_jobs=-1, num_leaves=57,\n",
       "               random_state=123)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.6, feature_fraction=0.6, learning_rate=0.01,\n",
       "               min_child_samples=10, n_estimators=600, n_jobs=-1, num_leaves=57,\n",
       "               random_state=123)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. Bayesian Optimization for LightGBM\n",
    "# =============================================================================\n",
    "\n",
    "def lgb_eval(learning_rate, num_leaves, feature_fraction, bagging_fraction, min_child_samples):\n",
    "    model = LGBMClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        num_leaves=int(num_leaves),\n",
    "        feature_fraction=feature_fraction,\n",
    "        bagging_fraction=bagging_fraction,\n",
    "        min_child_samples=int(min_child_samples),\n",
    "        n_estimators=400,\n",
    "        random_state=123,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    pred_prob = model.predict_proba(X_val_enc)[:, 1]\n",
    "    auc = roc_auc_score(y_val, pred_prob)\n",
    "\n",
    "    return auc\n",
    "\n",
    "params_bounds = {\n",
    "    \"learning_rate\": (0.01, 0.2),\n",
    "    \"num_leaves\": (20, 80),\n",
    "    \"feature_fraction\": (0.6, 1.0),\n",
    "    \"bagging_fraction\": (0.6, 1.0),\n",
    "    \"min_child_samples\": (10, 60)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lgb_eval,\n",
    "    pbounds=params_bounds,\n",
    "    random_state=123,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nRunning Bayesian Optimization...\\n\")\n",
    "optimizer.maximize(init_points=8, n_iter=20)\n",
    "\n",
    "best_params = optimizer.max[\"params\"]\n",
    "best_params[\"num_leaves\"] = int(best_params[\"num_leaves\"])\n",
    "best_params[\"min_child_samples\"] = int(best_params[\"min_child_samples\"])\n",
    "\n",
    "print(\"\\nBest params:\", best_params)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Train Final LightGBM on SMOTE data\n",
    "# =============================================================================\n",
    "\n",
    "final_model = LGBMClassifier(\n",
    "    n_estimators=600,\n",
    "    random_state=123,\n",
    "    n_jobs=-1,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfc41ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "\n",
      "================ FINAL LIGHTGBM RESULTS ================\n",
      "Confusion Matrix:\n",
      " [[19126  1378]\n",
      " [ 1627   732]]\n",
      "Accuracy : 0.8686\n",
      "Precision: 0.3469\n",
      "Recall   : 0.3103\n",
      "F1 Score : 0.3276\n",
      "AUC      : 0.6215\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. Evaluate on TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "pred_prob = final_model.predict_proba(X_test_enc)[:, 1]\n",
    "pred = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "prec = precision_score(y_test, pred)\n",
    "rec = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, pred)\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"\\n================ FINAL LIGHTGBM RESULTS ================\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"AUC      : {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19079413",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "562281b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== XGBOOST CLASSIFICATION (RANDOMIZED SEARCH CV) ===========\n",
      "\n",
      "scale_pos_weight: 8.691093095957187\n",
      "Running RandomizedSearchCV...\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "\n",
      "Best Parameters Found:\n",
      "{'colsample_bytree': 0.7447154622489256, 'eta': 0.05337001386700157, 'gamma': 0.08811421391664881, 'max_depth': 3, 'min_child_weight': 4, 'subsample': 0.63684197597803}\n",
      "\n",
      "Best CV AUC: 0.8226532648969487\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7447154622489256, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eta=0.05337001386700157, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.08811421391664881, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=600, n_jobs=-1,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7447154622489256, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eta=0.05337001386700157, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.08811421391664881, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=600, n_jobs=-1,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7447154622489256, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eta=0.05337001386700157, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.08811421391664881, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=600, n_jobs=-1,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=========== XGBOOST CLASSIFICATION (RANDOMIZED SEARCH CV) ===========\\n\")\n",
    "\n",
    "# ========================================================================\n",
    "# Compute scale_pos_weight (imbalance handling)\n",
    "# ========================================================================\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# ========================================================================\n",
    "# DEFINE PARAMETER DISTRIBUTIONS (professional approach)\n",
    "# ========================================================================\n",
    "param_dist = {\n",
    "    \"eta\": uniform(0.01, 0.19),              # learning rate\n",
    "    \"max_depth\": randint(3, 11),\n",
    "    \"min_child_weight\": randint(1, 21),\n",
    "    \"subsample\": uniform(0.6, 0.4),\n",
    "    \"colsample_bytree\": uniform(0.6, 0.4),\n",
    "    \"gamma\": uniform(0, 0.3),\n",
    "}\n",
    "\n",
    "# ========================================================================\n",
    "# BUILD BASE MODEL\n",
    "# ========================================================================\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=400,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=123,\n",
    "    n_jobs=-1\n",
    ")\n",
    "# ========================================================================\n",
    "# RANDOMIZED SEARCH CV\n",
    "# ========================================================================\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,                  # number of random combinations to try\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=123,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Running RandomizedSearchCV...\\n\")\n",
    "\n",
    "random_search.fit(X_train_enc, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(best_params)\n",
    "\n",
    "print(\"\\nBest CV AUC:\", random_search.best_score_)\n",
    "\n",
    "# ========================================================================\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMS\n",
    "# ========================================================================\n",
    "final_xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=600,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=123,\n",
    "    n_jobs=-1,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_xgb.fit(X_train_enc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed138948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ FINAL XGBOOST TEST RESULTS =================\n",
      "Confusion Matrix:\n",
      " [[14391  6113]\n",
      " [  527  1832]]\n",
      "Accuracy : 0.7096\n",
      "Precision: 0.2306\n",
      "Recall   : 0.7766\n",
      "F1 Score : 0.3556\n",
      "AUC      : 0.7392\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# MODEL EVALUATION ON TEST SET\n",
    "# ========================================================================\n",
    "pred_prob = final_xgb.predict_proba(X_test_enc)[:, 1]\n",
    "pred = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "prec = precision_score(y_test, pred)\n",
    "rec = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, pred)\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"\\n================ FINAL XGBOOST TEST RESULTS =================\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall   : {rec:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"AUC      : {auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
